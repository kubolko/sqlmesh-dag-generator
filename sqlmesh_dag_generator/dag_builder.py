"""
Airflow DAG builder module
"""
from typing import Dict, List, Optional
from textwrap import dedent, indent
from datetime import datetime, timedelta

from sqlmesh_dag_generator.config import DAGGeneratorConfig
from sqlmesh_dag_generator.models import DAGStructure, SQLMeshModelInfo


class AirflowDAGBuilder:
    """
    Builds Airflow DAG Python code from SQLMesh models.
    """

    def __init__(self, config: DAGGeneratorConfig, dag_structure: DAGStructure):
        self.config = config
        self.dag_structure = dag_structure

    def build(self) -> str:
        """
        Build the complete Airflow DAG Python file (static generation).

        Returns:
            Python code as a string
        """
        sections = [
            self._build_header(),
            self._build_imports(),
            self._build_config(),
            self._build_dag_definition(),
            self._build_tasks(),
            self._build_dependencies(),
        ]

        return "\n\n".join(sections)

    def build_dynamic(self) -> str:
        """
        Build a dynamic DAG that discovers SQLMesh models at runtime.

        This creates a single DAG file that works for any SQLMesh project.
        Models are discovered when Airflow parses the DAG, so no regeneration
        is needed when models change.

        Returns:
            Python code as a string
        """
        sections = [
            self._build_dynamic_header(),
            self._build_dynamic_imports(),
            self._build_dynamic_config(),
            self._build_dynamic_model_discovery(),
            self._build_dynamic_dag_definition(),
            self._build_dynamic_tasks(),
        ]

        return "\n\n".join(sections)

    def _build_header(self) -> str:
        """Build file header with metadata"""
        return dedent(f'''
            """
            Airflow DAG generated from SQLMesh project
            
            DAG ID: {self.config.airflow.dag_id}
            Generated: {datetime.now().isoformat()}
            
            This DAG was automatically generated by sqlmesh-dag-generator.
            DO NOT EDIT MANUALLY - changes will be overwritten.
            """
        ''').strip()

    def _build_imports(self) -> str:
        """Build import statements"""
        imports = [
            "from datetime import datetime, timedelta",
            "import logging",
            "",
            "from airflow import DAG",
        ]

        if self.config.generation.operator_type == "python":
            imports.append("from airflow.operators.python import PythonOperator")
        elif self.config.generation.operator_type == "bash":
            imports.append("from airflow.operators.bash import BashOperator")
        elif self.config.generation.operator_type == "kubernetes":
            imports.append("from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import KubernetesPodOperator")

        imports.extend([
            "",
            "from sqlmesh import Context",
            "",
            "logger = logging.getLogger(__name__)",
        ])

        return "\n".join(imports)

    def _build_config(self) -> str:
        """Build configuration section"""
        return dedent(f'''
            # SQLMesh Configuration
            SQLMESH_PROJECT_PATH = "{self.config.sqlmesh.project_path}"
            SQLMESH_ENVIRONMENT = "{self.config.sqlmesh.environment}"
            SQLMESH_GATEWAY = {f'"{self.config.sqlmesh.gateway}"' if self.config.sqlmesh.gateway else 'None'}
        ''').strip()

    def _build_dag_definition(self) -> str:
        """Build DAG definition"""
        default_args = self._format_default_args()

        schedule = f'"{self.config.airflow.schedule_interval}"' if self.config.airflow.schedule_interval else 'None'
        tags = repr(self.config.airflow.tags)
        description = f'"{self.config.airflow.description}"' if self.config.airflow.description else f'"SQLMesh DAG: {self.config.airflow.dag_id}"'

        return f"""# DAG Definition
dag = DAG(
    dag_id="{self.config.airflow.dag_id}",
    default_args={default_args},
    description={description},
    schedule_interval={schedule},
    start_date=datetime(2024, 1, 1),
    catchup={self.config.airflow.catchup},
    max_active_runs={self.config.airflow.max_active_runs},
    tags={tags},
)"""

    def _format_default_args(self) -> str:
        """Format default_args dictionary"""
        args = self.config.airflow.default_args.copy()

        # Ensure some basic defaults
        if 'owner' not in args:
            args['owner'] = 'sqlmesh'
        if 'retries' not in args:
            args['retries'] = 1
        if 'retry_delay' not in args:
            args['retry_delay'] = timedelta(minutes=5)

        # Format as Python dict
        lines = ["{"]
        for key, value in args.items():
            if key == 'retry_delay':
                # Handle timedelta - always output as timedelta object
                if isinstance(value, timedelta):
                    total_minutes = int(value.total_seconds() / 60)
                    value_str = f"timedelta(minutes={total_minutes})"
                elif isinstance(value, int):
                    value_str = f"timedelta(minutes={value})"
                else:
                    value_str = "timedelta(minutes=5)"
            elif isinstance(value, str):
                value_str = f'"{value}"'
            else:
                value_str = str(value)
            lines.append(f'    "{key}": {value_str},')
        lines.append("}")

        return "\n".join(lines)

    def _build_tasks(self) -> str:
        """Build task definitions"""
        task_defs = []

        # Build task for each model
        for model_name, model_info in self.dag_structure.models.items():
            task_def = self._build_single_task(model_info)
            task_defs.append(task_def)

        return "\n\n".join(task_defs)

    def _build_single_task(self, model_info: SQLMeshModelInfo) -> str:
        """Build a single task definition"""
        task_id = model_info.get_task_id()

        if self.config.generation.operator_type == "python":
            return self._build_python_task(model_info, task_id)
        elif self.config.generation.operator_type == "bash":
            return self._build_bash_task(model_info, task_id)
        else:
            return self._build_python_task(model_info, task_id)

    def _build_python_task(self, model_info: SQLMeshModelInfo, task_id: str) -> str:
        """Build a PythonOperator task"""
        function_name = f"execute_{task_id}"

        # Escape model name for use in strings
        model_name_escaped = model_info.name.replace('"', '\\"')

        # Build the execution function
        func_def = f'''def {function_name}(**context):
    """Execute SQLMesh model: {model_name_escaped}"""
    logger.info("Executing SQLMesh model: {model_name_escaped}")
    
    # Load SQLMesh context
    ctx = Context(
        paths=SQLMESH_PROJECT_PATH,
        gateway=SQLMESH_GATEWAY,
    )
    
    # Get time interval from Airflow context (Airflow 2.2+)
    # Use data_interval for proper incremental model handling
    start = context.get('data_interval_start') or context.get('execution_date')
    end = context.get('data_interval_end') or context.get('execution_date')
    
    # Run the specific model
    logger.info(f"Running model {model_name_escaped} for interval {{start}} to {{end}}")
    
    # Use SQLMesh's run method with model selection
    result = ctx.run(
        environment=SQLMESH_ENVIRONMENT,
        start=start,
        end=end,
        select_models=["{model_name_escaped}"],
    )
    
    logger.info(f"Model {model_name_escaped} completed successfully")
    return result'''

        # Build the operator
        operator_def = f'''{task_id} = PythonOperator(
    task_id="{task_id}",
    python_callable={function_name},
    dag=dag,
)'''

        return f"{func_def}\n\n{operator_def}"

    def _build_bash_task(self, model_info: SQLMeshModelInfo, task_id: str) -> str:
        """Build a BashOperator task"""
        bash_command = f"cd {self.config.sqlmesh.project_path} && sqlmesh run --select-models {model_info.name}"

        return dedent(f'''
            {task_id} = BashOperator(
                task_id="{task_id}",
                bash_command="{bash_command}",
                dag=dag,
            )
        ''').strip()

    def _build_dependencies(self) -> str:
        """Build task dependencies"""
        dep_lines = ["# Task Dependencies"]

        for model_name, model_info in self.dag_structure.models.items():
            if not model_info.dependencies:
                continue

            task_id = model_info.get_task_id()

            for dep_name in model_info.dependencies:
                # Check if dependency exists in our models
                if dep_name not in self.dag_structure.models:
                    continue

                dep_task_id = self.dag_structure.models[dep_name].get_task_id()
                dep_lines.append(f"{dep_task_id} >> {task_id}")

        if len(dep_lines) == 1:
            dep_lines.append("# No dependencies")

        return "\n".join(dep_lines)

    # ========================================================================
    # Dynamic DAG Generation Methods
    # ========================================================================

    def _build_dynamic_header(self) -> str:
        """Build header for dynamic DAG"""
        return f'''"""
Dynamic SQLMesh DAG - Auto-discovers models at runtime

DAG ID: {self.config.airflow.dag_id}
Generated: {datetime.now().isoformat()}

This DAG automatically discovers SQLMesh models at runtime.
Place this file in Airflow's dags/ folder ONCE and forget about it!
When SQLMesh models change, the DAG automatically updates.

DO NOT EDIT MANUALLY - changes will be overwritten.
"""'''

    def _build_dynamic_imports(self) -> str:
        """Build imports for dynamic DAG"""
        imports = [
            "from datetime import datetime, timedelta",
            "import logging",
            "",
            "from airflow import DAG",
            "from airflow.operators.python import PythonOperator",
            "from airflow.models import Variable",
            "from airflow.exceptions import AirflowException",
            "",
            "from sqlmesh import Context",
            "from sqlmesh.utils.errors import SQLMeshError",
            "",
            "logger = logging.getLogger(__name__)",
        ]
        return "\n".join(imports)

    def _build_dynamic_config(self) -> str:
        """Build configuration section for dynamic DAG using Airflow Variables"""
        # Use Airflow Variables for flexible configuration
        return f'''# SQLMesh Configuration (from Airflow Variables)
# Set these in Airflow UI: Admin > Variables
SQLMESH_PROJECT_PATH = Variable.get(
    "sqlmesh_project_path", 
    default_var="{self.config.sqlmesh.project_path}"
)
SQLMESH_ENVIRONMENT = Variable.get(
    "sqlmesh_environment", 
    default_var="{self.config.sqlmesh.environment}"
)
SQLMESH_GATEWAY = Variable.get(
    "sqlmesh_gateway", 
    default_var={f'"{self.config.sqlmesh.gateway}"' if self.config.sqlmesh.gateway else 'None'}
)

logger.info(f"SQLMesh Project Path: {{SQLMESH_PROJECT_PATH}}")
logger.info(f"SQLMesh Environment: {{SQLMESH_ENVIRONMENT}}")'''

    def _build_dynamic_model_discovery(self) -> str:
        """Build model discovery section"""
        return '''# Discover SQLMesh models at DAG parse time
logger.info("Loading SQLMesh context and discovering models...")
try:
    ctx = Context(
        paths=SQLMESH_PROJECT_PATH,
        gateway=SQLMESH_GATEWAY,
    )
    
    # Extract model information
    discovered_models = {}
    for model_name, model in ctx.models.items():
        discovered_models[model_name] = {
            "fqn": model.fqn,
            "name": str(model.name),
            "dependencies": [str(dep.name) for dep in model.depends_on],
        }
    
    logger.info(f"✓ Discovered {len(discovered_models)} SQLMesh models")
    
except Exception as e:
    logger.error(f"Failed to load SQLMesh context: {e}")
    # Create empty dict to prevent DAG parse errors
    discovered_models = {}
    logger.warning("DAG will be created with no tasks")'''

    def _build_dynamic_dag_definition(self) -> str:
        """Build DAG definition for dynamic DAG"""
        default_args = self._format_default_args()
        schedule = f'"{self.config.airflow.schedule_interval}"' if self.config.airflow.schedule_interval else 'None'
        tags = repr(self.config.airflow.tags + ['dynamic', 'auto-generated'])
        description = f'"{self.config.airflow.description}"' if self.config.airflow.description else f'"Dynamic SQLMesh DAG: {self.config.airflow.dag_id}"'

        return f'''# Create DAG
with DAG(
    dag_id="{self.config.airflow.dag_id}",
    start_date=datetime(2024, 1, 1),
    schedule_interval={schedule},
    catchup={self.config.airflow.catchup},
    max_active_runs={self.config.airflow.max_active_runs},
    default_args={default_args},
    description={description},
    tags={tags},
) as dag:
    
    tasks = {{}}
    
    # Create task for each discovered model
    for model_name, model_info in discovered_models.items():
        # Sanitize task ID (remove quotes, dots, etc.)
        task_id = f"sqlmesh_{{model_name.replace('.', '_').replace('"', '').replace("'", '').replace(' ', '_')}}"
        # Remove consecutive underscores
        while "__" in task_id:
            task_id = task_id.replace("__", "_")
        task_id = task_id.strip("_")
        
        # Create callable for this model
        def make_callable(model_fqn, model_display_name):
            """Factory function to create model-specific callable"""
            def execute_model(**context):
                """Execute SQLMesh model"""
                logger.info(f"Executing SQLMesh model: {{model_display_name}}")
                
                try:
                    # Load fresh SQLMesh context
                    run_ctx = Context(
                        paths=SQLMESH_PROJECT_PATH,
                        gateway=SQLMESH_GATEWAY,
                    )
                    
                    # Get time interval (Airflow 2.2+)
                    # data_interval_start/end provides correct time range for incremental models
                    # Falls back to execution_date for backward compatibility with Airflow < 2.2
                    start = context.get('data_interval_start') or context.get('execution_date')
                    end = context.get('data_interval_end') or context.get('execution_date')
                    
                    logger.info(f"Running model {{model_display_name}} for interval {{start}} to {{end}}")
                    
                    # Run the specific model with proper time range
                    result = run_ctx.run(
                        environment=SQLMESH_ENVIRONMENT,
                        start=start,
                        end=end,
                        select_models=[model_fqn],
                    )
                    
                    logger.info(f"✓ Model {{model_display_name}} completed successfully")
                    return result
                    
                except SQLMeshError as e:
                    logger.error(f"✗ SQLMesh error in model {{model_display_name}}: {{e}}")
                    raise AirflowException(f"SQLMesh run failed: {{e}}")
                except Exception as e:
                    logger.error(f"✗ Unexpected error in model {{model_display_name}}: {{e}}")
                    raise
            
            return execute_model
        
        # Create PythonOperator
        task = PythonOperator(
            task_id=task_id,
            python_callable=make_callable(model_info["fqn"], model_info["name"]),
        )
        
        tasks[model_name] = task
    
    # Set up dependencies based on SQLMesh lineage
    for model_name, model_info in discovered_models.items():
        if model_name not in tasks:
            continue
        
        current_task = tasks[model_name]
        
        for dep_name in model_info["dependencies"]:
            if dep_name in tasks:
                # Create dependency: upstream >> downstream
                tasks[dep_name] >> current_task
    
    logger.info(f"✓ DAG created with {{len(tasks)}} tasks")'''

    def _build_dynamic_tasks(self) -> str:
        """Build tasks section for dynamic DAG (already included in DAG definition)"""
        # Tasks are built dynamically inside the with DAG block
        # This method is here for API consistency
        return ""

